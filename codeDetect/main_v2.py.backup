"""
Living Documentation Engine - Code Change Detection
Main Entry Point (US-18, US-19)

This module orchestrates the entire analysis pipeline:
1. Git context extraction (US-1, US-2, US-3)
2. File filtering (US-4, US-5)
3. Multi-language parsing (US-6, US-7, US-8, US-9, US-10, US-11, US-12)
4. Impact scoring (US-13, US-14, US-15, US-16, US-17)
5. JSON output generation (US-18, US-19)
"""

import sys
import json
import os
import datetime
import yaml
from typing import Dict, List, Any, Optional

from src.git_manager import GitManager
from src.file_filter import FileFilter, ConfigLoader
from src.scorers import SeverityCalculator, ComplexityScorer
from src.syntax_checker import SyntaxChecker

# Import parsers - prefer tree-sitter, fall back to regex
try:
    from src.parsers.tree_sitter_engine import parse_code as ts_parse
    TREE_SITTER_MODE = True
except ImportError:
    TREE_SITTER_MODE = False

from src.parsers.java_parser import JavaParser
from src.parsers.ts_parser import TSParser
from src.parsers.js_parser import JSParser
from src.parsers.schema_detector import SchemaDetector


class LivingDocEngine:
    """
    Main engine for code change detection and analysis.
    """

    def __init__(self, repo_path: str, config_path: Optional[str] = None):
        self.repo_path = repo_path
        self.config = self._load_config(config_path)
        self.git_manager = GitManager(repo_path)

    def _load_config(self, config_path: Optional[str]) -> Dict:
        """Load configuration from YAML file (US-4)."""
        default_config = {
            "ignore_patterns": [],
            "critical_paths": [],
            "api_patterns": {},
            "schema_patterns": {},
            "severity_weights": {
                "api_change": 10,
                "schema_change": 10,
                "config_change": 7,
                "business_logic": 5
            }
        }

        if config_path and os.path.exists(config_path):
            try:
                with open(config_path, 'r') as f:
                    loaded = yaml.safe_load(f)
                    if loaded:
                        default_config.update(loaded)
            except Exception as e:
                print(f"Warning: Could not load config: {e}")
        else:
            # Try default location
            default_path = os.path.join(os.path.dirname(__file__), 'config.yaml')
            if os.path.exists(default_path):
                try:
                    with open(default_path, 'r') as f:
                        loaded = yaml.safe_load(f)
                        if loaded:
                            default_config.update(loaded)
                except:
                    pass

        return default_config

    def analyze(self) -> Dict[str, Any]:
        """
        Run the complete analysis pipeline.

        Returns:
            Complete impact report dictionary
        """
        report = {
            "meta": {
                "generated_at": datetime.datetime.utcnow().isoformat() + "Z",
                "tool_version": "2.0.0",
                "parser_mode": "tree-sitter" if TREE_SITTER_MODE else "regex"
            },
            "context": {},
            "analysis_summary": {
                "total_files": 0,
                "analyzed_files": 0,
                "skipped_files": 0,
                "highest_severity": "PATCH",
                "breaking_changes_detected": False,
                "total_complexity": 0
            },
            "changes": [],
            "dependency_graph": {},  # US-15
            "api_surface": [],       # US-13
            "schema_changes": []     # US-14
        }

        try:
            # US-1, US-19: Git Context
            report["context"] = self.git_manager.get_metadata()

            # US-2, US-3: Get changed files
            changes_list = self.git_manager.get_changed_files()
            report["analysis_summary"]["total_files"] = len(changes_list)

            severity_rank = {"PATCH": 1, "MINOR": 2, "MAJOR": 3}
            max_severity = 0
            total_complexity = 0

            for change in changes_list:
                file_path = change["path"]

                record = self._create_change_record(file_path, change["change_type"])

                # US-4, US-5: Filter and safety check
                if not FileFilter.is_safe_to_read(file_path, self.config.get("ignore_patterns", [])):
                    record["is_binary"] = True
                    record["note"] = "Skipped: binary or ignored file"
                    report["changes"].append(record)
                    report["analysis_summary"]["skipped_files"] += 1
                    continue

                # Read content (US-5: Binary Safety)
                content = self.git_manager.get_file_content(file_path)
                if content is None:
                    record["is_binary"] = True
                    record["note"] = "Skipped: could not read file"
                    report["changes"].append(record)
                    report["analysis_summary"]["skipped_files"] += 1
                    continue

                ext = os.path.splitext(file_path)[1].lower()
                record["language"] = self._detect_language(ext)

                # US-11: Syntax Check
                syntax_error = SyntaxChecker.check(file_path, content)
                record["syntax_error"] = syntax_error
                if syntax_error:
                    record["note"] = "Warning: syntax error detected, partial analysis"

                # Parse features using tree-sitter or regex fallback
                features = self._parse_file(content, ext, file_path)
                record["features"] = features

                # US-12: Extract comments
                if features.get("comments"):
                    record["comments"] = features["comments"][:5]  # Limit to 5

                # US-12: Extract docstrings
                if features.get("docstrings"):
                    record["docstrings"] = features["docstrings"][:3]

                # US-14: Schema detection
                schema_tags = SchemaDetector.analyze(file_path, content)
                if schema_tags:
                    record["schema_tags"] = schema_tags
                    report["schema_changes"].append({
                        "file": file_path,
                        "tags": schema_tags,
                        "change_type": change["change_type"]
                    })

                # US-13: API endpoints
                if features.get("api_endpoints"):
                    for endpoint in features["api_endpoints"]:
                        endpoint["file"] = file_path
                        report["api_surface"].append(endpoint)

                if features.get("api_routes"):
                    for route in features["api_routes"]:
                        route["file"] = file_path
                        report["api_surface"].append(route)

                # US-15: Dependency graph
                imports = features.get("imports", []) or features.get("dependencies", [])
                if imports:
                    report["dependency_graph"][file_path] = imports

                # US-16, US-17: Scoring
                complexity = ComplexityScorer.calculate(features)
                severity = SeverityCalculator.assess(ext, features, schema_tags, self.config)

                record["complexity_score"] = complexity
                record["severity"] = severity
                total_complexity += complexity

                # Update summary
                if severity_rank.get(severity, 0) > max_severity:
                    max_severity = severity_rank[severity]
                    report["analysis_summary"]["highest_severity"] = severity

                if severity == "MAJOR":
                    report["analysis_summary"]["breaking_changes_detected"] = True

                report["changes"].append(record)
                report["analysis_summary"]["analyzed_files"] += 1

            report["analysis_summary"]["total_complexity"] = total_complexity

        except Exception as e:
            report["error"] = str(e)
            import traceback
            report["traceback"] = traceback.format_exc()

        return report

    def _create_change_record(self, file_path: str, change_type: str) -> Dict:
        """Create a new change record."""
        return {
            "file": file_path,
            "change_type": change_type,
            "language": None,
            "severity": "PATCH",
            "complexity_score": 0,
            "is_binary": False,
            "syntax_error": False,
            "features": {}
        }

    def _detect_language(self, ext: str) -> Optional[str]:
        """Detect language from file extension."""
        language_map = {
            '.java': 'java',
            '.js': 'javascript',
            '.jsx': 'javascript',
            '.ts': 'typescript',
            '.tsx': 'typescript',
            '.py': 'python',
            '.go': 'go',
            '.rs': 'rust',
            '.rb': 'ruby',
            '.php': 'php',
            '.c': 'c',
            '.cpp': 'cpp',
            '.cs': 'csharp'
        }
        return language_map.get(ext)

    def _parse_file(self, content: str, ext: str, file_path: str) -> Dict:
        """
        Parse file content and extract features.
        Uses tree-sitter if available, falls back to regex parsers.
        """
        features = {}

        # Try tree-sitter first
        if TREE_SITTER_MODE:
            try:
                result = ts_parse(content, ext)
                if result.get("features"):
                    return result["features"]
            except Exception as e:
                print(f"Tree-sitter parsing failed for {file_path}: {e}")

        # Fallback to regex parsers
        if ext == '.java':
            features = JavaParser.analyze(content)
        elif ext in ['.ts', '.tsx', '.js', '.jsx']:
            features = TSParser.analyze(content)
            # Merge with JSParser for React detection
            js_findings = JSParser.analyze(content)
            features["react_patterns"] = js_findings
        elif ext == '.py':
            features = self._parse_python_regex(content)

        return features

    def _parse_python_regex(self, content: str) -> Dict:
        """Regex-based Python parsing fallback (US-10)."""
        import re

        features = {
            "functions": [],
            "classes": [],
            "decorators": [],
            "imports": [],
            "docstrings": [],
            "complexity_nodes": 0
        }

        # Functions
        features["functions"] = re.findall(r'def\s+(\w+)\s*\(', content)

        # Classes
        features["classes"] = re.findall(r'class\s+(\w+)\s*[:\(]', content)

        # Decorators
        features["decorators"] = re.findall(r'@(\w+(?:\.\w+)*)', content)

        # Imports
        import_simple = re.findall(r'^import\s+(\S+)', content, re.MULTILINE)
        import_from = re.findall(r'^from\s+(\S+)\s+import', content, re.MULTILINE)
        features["imports"] = import_simple + import_from

        # Docstrings (triple quotes)
        docstrings = re.findall(r'"""(.*?)"""', content, re.DOTALL)
        features["docstrings"] = [{"text": d[:200]} for d in docstrings[:3]]

        # Complexity (count branching keywords)
        complexity_keywords = ['if ', 'elif ', 'for ', 'while ', 'try:', 'except', 'with ']
        for kw in complexity_keywords:
            features["complexity_nodes"] += content.count(kw)

        return features


def main():
    """Main entry point."""
    if len(sys.argv) < 2:
        print(json.dumps({
            "error": "Usage: python main.py <repo_path> [config_path]",
            "example": "python main.py /path/to/repo config.yaml"
        }, indent=2))
        sys.exit(1)

    repo_path = sys.argv[1]
    config_path = sys.argv[2] if len(sys.argv) > 2 else None

    try:
        engine = LivingDocEngine(repo_path, config_path)
        report = engine.analyze()

        # US-18: JSON Output
        output_path = os.path.join(os.path.dirname(__file__), 'change_report.json')
        with open(output_path, 'w') as f:
            json.dump(report, f, indent=2)

        print(json.dumps(report, indent=2))

    except Exception as e:
        error_report = {
            "status": "error",
            "message": str(e),
            "type": type(e).__name__
        }
        print(json.dumps(error_report, indent=2))
        sys.exit(1)


if __name__ == "__main__":
    main()
